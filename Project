# -------------------------------
# Read file from HDFS
# -------------------------------

hdfs_path = f"{hdfs_file_load_path}/{file_name}*"
raw = spark.read.text(hdfs_path)

lines = raw.collect()

header = lines[0].value
footer = lines[-1].value
data_lines = lines[1:-1]

print(f"Header: {header}")
print(f"Footer: {footer}")

# -------------------------------
# Parse header/footer info
# -------------------------------

header_parts = header.split(field_separator)
header_filedate = header_parts[2][:8]
print(f"Header File Date: {header_filedate}")

footer_parts = footer.split(field_separator)
footer_count = int(footer_parts[1])
footer_chksum = int(footer_parts[2])

print(f"Footer Count: {footer_count}")
print(f"Footer Checksum: {footer_chksum}")

# -------------------------------
# Convert data rows to DataFrame with fixed column count
# -------------------------------

expected_cols = 38   # âœ… hardcoded column count

parsed_rows = []

for row in data_lines:
    parts = row.value.split(field_separator)

    # Pad missing columns with NULL
    if len(parts) < expected_cols:
        parts = parts + [None] * (expected_cols - len(parts))

    # Trim extra columns if any
    elif len(parts) > expected_cols:
        parts = parts[:expected_cols]

    parsed_rows.append(parts)

# Create DataFrame
df_raw = spark.createDataFrame(parsed_rows)

print(f"Source Count: {df_raw.count()}")
df_raw.show(5)

# -------------------------------
# Compute checksum
# -------------------------------

from pyspark.sql.functions import sha2, concat_ws

checksum_df = df_raw.withColumn(
    "checksum",
    sha2(concat_ws("|", *df_raw.columns), 256)
)

checksum_df.show(5)

final_checksum = checksum_df.selectExpr(
    "sha2(concat_ws('|', collect_list(checksum)), 256) as final_checksum"
).collect()[0]["final_checksum"]

print(f"Computed Checksum: {final_checksum}")
