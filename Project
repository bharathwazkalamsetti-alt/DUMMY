# ------------------------------------------------------------
# 4. Read file from HDFS into DataFrame (Same format as teammate)
# ------------------------------------------------------------

hdfs_path = f"{hdfs_file_load_path}/{file_name}*"
raw = spark.read.text(hdfs_path)

# Collect lines (header + body + footer)
lines = raw.collect()

# Header is always first line
header = lines[0].value

# Footer is always last line
footer = lines[-1].value

# Everything between header and footer are data rows
data_lines = lines[1:-1]

print(f"Header: {header}")
print(f"Footer: {footer}")

# -----------------------------
# Parse footer to get row count
# -----------------------------
footer_parts = footer.split(field_separator)
footer_count = int(footer_parts[1])
print(f"Footer Count: {footer_count}")

# -----------------------------
# Convert raw rows into DataFrame
# -----------------------------
df_raw = spark.createDataFrame(
    [row.value.split(field_separator) for row in data_lines],
    schema=None
)

# Count number of raw rows
source_count = df_raw.count()
print(f"Source Count: {source_count}")
