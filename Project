# set expected_cols = 38, or max schema length from your JSON if you prefer
        expected_cols = 38
        parsed_rows = []
        for line in data_lines:
            parts = line.split(field_separator)
            parts = [p if p != "" else None for p in parts]  # blanks -> null
            if len(parts) < expected_cols:
                parts += [None] * (expected_cols - len(parts))
            elif len(parts) > expected_cols:
                parts = parts[:expected_cols]
            parsed_rows.append(parts)

        df_raw = spark.createDataFrame(parsed_rows)
        source_count = df_raw.count()

        print(f"Header File Date: {header_filedate}")
        print(f"Footer Count: {footer_count}")
        print(f"Footer Checksum: {footer_chksum}")
        print(f"Source Count: {source_count}")
        df_raw.show(5, truncate=False)

        # continue with validation and Step 6...

    except Exception as e:
        print(f"Error: {e}")
        if audit_id:
            update_audit_log(spark, audit_id, "FAILED", str(e), 0, start_time)
        raise
