# ------------ CONFIG ------------
JSON_CONFIG_FILE = Path(__file__).parents[2] / "trades" / "config" / "ingestion_config.json"

def initialize_spark(project_name, job_id, env):
    return (
        SparkSession.builder
        .master("yarn")
        .appName(f"{project_name}_{job_id}")
        .config("spark.app.env", env)
        .getOrCreate()
    )

def main():
    print("Starting GIW File Validation and Stage Load Process")
    project_name = "PUP-TRADE"
    job_id = "10"
    env = "dev"

    spark = initialize_spark(project_name, job_id, env)
    start_time = datetime.datetime.now()
    audit_id = None

    try:
        # 1. Get the job configuration data
        print(f"Fetching job config data for project: {project_name}, job_id: {job_id}")
        job_config_data = fetch_config_params(spark, project_name, job_id)
        print(f"Job Config Data: {job_config_data}")
        if not job_config_data:
            print(f"No config found for job ID {job_id} in project {project_name}")
            return

        for row in job_config_data:
            project_name = row.PROJECT_NAME
            job_id = row.JOB_ID
            job_name = row.JOB_NAME
            source_system = row.SOURCE_SYSTEM
            file_name = row.FILE_NAME
            file_source_path = row.FILE_SOURCE_PATH
            hdfs_file_load_path = row.HDFS_FILE_LOAD_PATH
            file_archive_path = row.FILE_ARCHIVE_PATH
            file_load_failed = row.FILE_LOAD_FAILED
            file_extension = row.FILE_EXTENSION
            field_separator = row.FIELD_SEPARATOR
            target_table = row.TARGET_TABLE
            target_connection_name = row.TARGET_CONNECTION_NAME

        # 2. Transfer file from NAS to HDFS
        print(f"Copying file from NAS to HDFS: {file_source_path} to {hdfs_file_load_path}")
        complete_local_file_path, actual_file_name, file_last_modified_time = copy_file_nas_to_hdfs(
            spark, file_source_path, hdfs_file_load_path, file_name
        )
        print(f"File copied successfully: {actual_file_name}, Last Modified Time: {file_last_modified_time}")
        print(f"Complete Local File Path: {complete_local_file_path}")

        # 3. Audit - Start
        print("Starting Audit Log Entry")
        audit_id = insert_audit_log(
            spark, project_name, job_id, job_name, source_system, actual_file_name,
            file_last_modified_time, start_time
        )
        print(f"Audit Log Entry Created with ID: {audit_id}")

        # 4. Read file from HDFS
        hdfs_path = f"{hdfs_file_load_path}/{file_name}*"
        raw = spark.read.text(hdfs_path)

        lines = raw.rdd.map(lambda r: r.value).collect()
        header = lines[0]
        footer = lines[-1]
        data_lines = lines[1:-1]

        print(f"Header: {header}")
        print(f"Footer: {footer}")

        header_parts = header.split(field_separator)
        header_filedate = header_parts[2][0:8]
        footer_parts = footer.split(field_separator)
        footer_count = int(footer_parts[1])
        footer_chksum = int(footer_parts[2])

        # set expected_cols = 38, or max schema length from your JSON if you prefer
        expected_cols = 38
        parsed_rows = []
        for line in data_lines:
            parts = line.split(field_separator)
            parts = [p if p != "" else None for p in parts]  # blanks -> null
            if len(parts) < expected_cols:
                parts += [None] * (expected_cols - len(parts))
            elif len(parts) > expected_cols:
                parts = parts[:expected_cols]
            parsed_rows.append(parts)

        df_raw = spark.createDataFrame(parsed_rows)
        source_count = df_raw.count()

        print(f"Header File Date: {header_filedate}")
        print(f"Footer Count: {footer_count}")
        print(f"Footer Checksum: {footer_chksum}")
        print(f"Source Count: {source_count}")
        df_raw.show(5, truncate=False)

        # continue with validation and Step 6...

    except Exception as e:
        print(f"Error: {e}")
        if audit_id:
            update_audit_log(spark, audit_id, "FAILED", str(e), 0, start_time)
        raise
