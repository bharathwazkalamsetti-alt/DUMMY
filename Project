# Step 11: Delete existing data for this target table
print(f"Deleting existing data from target table: {target_table}")
delete_target_table(spark, target_table, source_system)


# Step 12: Write to target table
print(f"Writing to target table: {target_table}")
final_df.write.mode("append").saveAsTable(target_table)


# Step 13: Target Count
target_count = get_table_count(spark, target_table, source_system)
print(f"Target Count for {target_table}: {target_count}")


# Step 16: Audit Success for this segment
update_audit_log(spark, audit_id, "SUCCESS", source_count, target_count)
print(f"Audit updated successfully for {segment_key}")


# Step 17: Archive File after all segments processed
archive_file_from_hdfs(spark, hdfs_file_load_path, actual_file_name, file_archive_path)
print(f"File archived to: {file_archive_path}")


# Optional HDFS cleanup
print(f"Cleaning HDFS path: {hdfs_file_load_path}")
fs = spark._jvm.org.apache.hadoop.fs.FileSystem.get(spark._jsc.hadoopConfiguration())
fs.delete(spark._jvm.org.apache.hadoop.fs.Path(hdfs_file_load_path), True)


print("Trade ingestion job completed successfully.")


except Exception as e:
print(f"Error occurred: {str(e)}")
if audit_id:
update_audit_log(spark, audit_id, "FAILED", 0, 0)
try:
archive_file_from_hdfs(spark, hdfs_file_load_path, file_name, file_load_failed)
except:
pass
